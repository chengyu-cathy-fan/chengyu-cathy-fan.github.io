<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <meta name="google-site-verification" content="anLNFr_oDVKDmuvPel0drhtFdkgSXsdZba_IDbphNZc" />
  <meta name="description" content="Cathy Fan -- Computer Science & AI Research, Colby College">
  <title>Chengyu Cathy Fan</title>
  <link rel="icon" href="assets/favicon_monkey.png" type="image/png" />
  <link rel="stylesheet" href="style.css" />

  <!-- Open Graph -->
  <meta property="og:title" content="Chengyu 'Cathy' Fan -- Computer Science & AI Research" />
  <meta property="og:description" content="Undergraduate Researcher at Colby College. Actively seeking a PhD position for Fall 2026." />
  <meta property="og:image" content="profile.jpg" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://chengyu-cathy-fan.github.io/" />

  <!-- Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Person",
    "name": "Chengyu Fan",
    "alternateName": ["Cathy Fan", "Chengyu Cathy Fan", "Cathy Fan Colby College", "Chengyu Fan Colby College"],
    "jobTitle": "Undergraduate Researcher",
    "affiliation": {
      "@type": "CollegeOrUniversity",
      "name": "Colby College"
    },
    "sameAs": [
      "https://scholar.google.com/citations?user=ijsIo2wAAAAJ&hl=en", 
      "https://www.linkedin.com/in/chengyu-cathy-fan-38578b245/"
    ]
  }
  </script>
</head>
<body>
    <div class="announcement-banner">
    ðŸ¤— Actively seeking a PhD position for Fall 2026</a>
    </div>

  <header>
    <div class="container">
      <img src="profile.jpg" alt="Profile Photo" class="profile-photo" />
      <h1>Chengyu "Cathy" Fan</h1>
      <p class="subtitle">Devoted in technology and human progress</p>

      <button class="cv-button" onclick="openModal()">Show CV</button>

      <nav>
        <ul>
          <li><a href="#about">About</a></li>
          <li><a href="#research">Research</a></li>
          <li><a href="#publications">Publications</a></li>
          <li>
            <a href="https://scholar.google.com/citations?user=ijsIo2wAAAAJ&hl=en" target="_blank">
              Google Scholar
            </a>
          </li>
        </ul>
      </nav>
    </div>
  </header>

  <main class="container">
    <section id="about">
        <h2>About Me </h2>
        <p style="margin-bottom: 1.5em;">
        Hi, I'm Cathy (or go by my legal name: Chengyu, which is a little tricky to <a href="https://translate.google.com/?hl=zh-CN&sl=zh-TW&tl=zh-CN&text=%E9%AA%8B%E9%92%B0&op=translate" target="_blank" class="name-link">pronounce</a>). I am a senior undergraduate student at Colby College majoring in Computer Science (AI concentration) and Mathematical Sciences, 
        with a minor in Statistics.
        </p>

        <p style="margin-bottom: 1.5em;">
        My current research focuses on improving the <strong>reliability and robustness of human pose estimation systems</strong> by tackling two challenges: annotation quality and edge cases in training data (e.g. occlusion) under the supervision of   
        <a href="https://sites.google.com/view/tahiyachowdhury" target="_blank" class="prof-link">
              Dr. Tahiya Chowdhury
        </a>. This project, which I will extend into my senior honors thesis, contributes to multimodal interaction by improving how systems interpret and respond to complex human body movements.  
        In addition, I am going to work under 
        <a href="https://stacyadoore.com/" target="_blank" class="prof-link">
          Dr. Stacy Doore 
        </a> on <strong>machine learning methods for generating accessible art descriptions in museums for people with visual impairments</strong>.
        </p>

        <p style="margin-bottom: 1.5em;">
        I'm also grateful to have had the opportunity to work on research projects in <strong>computer vision</strong>, <strong>LLM-driven agent simulation</strong>, and <strong>multimodal interaction for conversation analysis</strong> under the guidance of 
        <!-- link to be added -->
        <a href="https://amandastent.net/" target="_blank" class="prof-link">
          Dr. Amanda Stent
        </a>,
        <!-- link to be verified -->
        <a href="https://share.google/wBI46g5uRWnLMqWFZ" target="_blank" class="prof-link">
            Dr. Michael Yankoski
        </a>, 
        <!-- link to be verified -->
        <a href="https://www.wm.edu/as/data-science/people/ford-trenton.php" target="_blank" class="prof-link">
          Dr. Trenton Ford
        </a>, and 
        <!-- link to be verified -->
        <a href="https://veroromero.weebly.com/" target="_blank" class="prof-link">
          Dr. Veronica Romero
        </a>.
        </p>

        <p style="margin-bottom: 1.5em;">
        Looking ahead, I am seeking a PhD position starting in Fall 2026, where I hope to further explore research in computer vision, multimodal AI, responsible AI and accessibility design. If you are a faculty member, or know of an open position in a lab, I would love to connect.
        </p>
     
    </section>

    <section id="research">
    <h2 style="text-align: center; margin-bottom: 1em;">Research</h2>

    <div class="research-cards">
      <!-- Card 1 -->
      <div class="research-card">
        <div class="research-media">
          <img src="assets/images/multimodal_coordination.png" alt="Movement Coordination in Dyadic Conversations">
        </div>
        <div class="research-text">
          <h3>Movement Coordination in Dyadic Conversations</h3>
          <p>This research, combining computer science and psychology, adopted methods such as Cross-recurrence Quantification Analysis (CRQA), Dynamic Time Warping (DTW), and Convergent Cross Mapping (CCM) to quantify human movement coordination.</p>
        </div>
      </div>

      <!-- Card 2 -->
      <div class="research-card">
      <div class="research-media">
        <img src="assets/images/Comp-husim_demo.png" alt="Comp-Husim Agent" class="research-image">

       <!-- Supplementary video buttons -->
      <div class="video-links">
        <button onclick="window.open('https://drive.google.com/file/d/18r9j_BfTWDaMjIIm6Sckn5yqNTbhZ_xP/view?usp=drive_link', '_blank')">Demo 1</button>
        <button onclick="window.open('https://drive.google.com/file/d/1oOZD5ZQI3gMqaBTmGVOdeBOzRPOKzFCr/view?usp=drive_link', '_blank')">Demo 2</button>
        <button onclick="window.open('https://drive.google.com/file/d/116BbE0sQc4uZgGjK2PToRoehP8ACwZzJ/view?usp=drive_link', '_blank')">Demo 3</button>
      </div>
      </div>
       
      <div class="research-text">
        <h3>Comp-Husim: LLM-based agent simulation</h3>
        <p>
          At Colby's Davis Institute for AI, I helped build a persistent multi-agent simulation
          platform (Comp-HuSim: Complex Human Simulation) using large language models to simulate
          agents with digital personalities.
        </p>
      </div>
    </div>

     

      <!-- Card 3 -->
      <div class="research-card">
      <div class="research-media">
        <video controls poster="assets/images/pose_estimation.png" width="100%">
          <source src="assets/videos/pose_demo.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
      <div class="research-text">
        <h3>Pose Estimation and Responsible Multimodal Interaction</h3>
        <p>
          Currently integrating pose estimation techniques with ethical AI principles 
          as part of an ongoing research project in Colby's HUMANE Lab, including proposing 
          an occlusion index for robustness assessment.
        </p>
      </div>
    </div>


      <!-- Card 4 -->
      <div class="research-card">
        <div class="research-media">
          <img src="assets/images/AllenIsland_trash.png" alt="AI for Environmental Sustainability">
        </div>
        <div class="research-text">
          <h3>AI for Environmental Sustainability</h3>
          <p>Developed a drone image dataset (~1k images) and trained a computer vision model to detect coastal litter in Maine, achieving ~92% accuracy and supporting local recycling efforts.</p>
        </div>
      </div>
    </div>
  </section>


  <section id="publications">
  <h2>Publications</h2>
  <p><small>Note: * indicates equal contribution</small></p>
   <div class="publication-entry">
    <div class="pub-meta">
      <p class="pub-title">
        PoseDoc: An Interactive Tool for Efficient Keypoint Annotation in Human Pose Estimation
      </p>
      <p class="pub-authors"><strong>Fan, C.</strong> & Chowdhury, T.</p>
      <p class="pub-venue"><em>ACM ICMI '25</em> (Accepted, to appear)</p>
    </div>
  </div>

  <div class="publication-entry">
    <div class="pub-meta">
      <p class="pub-title">
        When Pose Estimation Fails: Measuring Occlusion for Reliable Multimodal Interaction
      </p>
      <p class="pub-authors"><strong>Fan, C.</strong> & Chowdhury, T.</p>
      <p class="pub-venue"><em>ACM ICMI '25</em> (Accepted, to appear)</p>
    </div>
  </div>

  <div class="publication-entry">
    <div class="pub-meta">
      <p class="pub-title">
        <a href="https://doi.org/10.1145/3686215.3690149" target="_blank">
          Towards Multimodality: Comparing Quantifications of Movement Coordination
        </a>
      </p>
      <p class="pub-authors"><strong>Fan, C.</strong>, Romero, V., Paxton, A., & Chowdhury, T.</p>
      <p class="pub-venue"><em>ACM ICMI '24</em></p>
    </div>
  </div>

  <div class="publication-entry">
    <div class="pub-meta">
      <p class="pub-title">
        <a href="https://doi.org/10.1145/3631700.3664883" target="_blank">
          Comp-HuSim: Persistent Digital Personality Simulation Platform
        </a>
      </p>
      <p class="pub-authors"><strong>Fan, C.*</strong>, Tariq, Z*., Bhuiyan, N. S., Yankoski, M. G., & Ford, T. W.</p>
      <p class="pub-venue"><em>ACM UMAP '24</em></p>
    </div>
  </div>

</section>

    
  </main>

  <footer>
    <div class="container">
      <p>&copy; 2025 Cathy Fan. Last updated: August 2025.</p>
    </div>
  </footer>

  <!-- Back to Top Button -->
  <button id="backToTop" title="Back to Top">â†‘</button>

  <!-- CV Modal -->
  <div id="cvModal" class="modal">
    <div class="modal-content">
      <span class="close" id="cvClose">&times;</span>
      <h3>CV Access</h3>
      <p class="prompt">Unscramble the word: YMNOEK</p>
      <p class="note" style="font-size: 0.85em; color: #666; margin-top: 10px;">
      Note: Your answer is <strong>not</strong> case-sensitive. If you prefer, you can <a href="protected/Cathy_CV_2508.pdf" target="_blank">skip the puzzle</a> and view the CV directly.
      </p>
      <input type="text" id="cvAnswer" placeholder="Enter your answer" />
      <button class="cv-submit" onclick="checkCVAnswer()">Submit</button>
      <p id="cvError" style="color: red; display: none; margin-top: 15px;">Try again! Hint: An animal</p>
      <p id="cvSuccess" style="color: green; display: none; margin-top: 15px;">Genius!</p>

      <div id="cvLink" style="display: none; margin-top: 15px;">
        <a href="protected/Cathy_CV_2508.pdf" class="cv-button" target="_blank">View CV</a>
      </div>
    </div>
  </div>

  <script>
    document.addEventListener("DOMContentLoaded", function () {
      const sections = document.querySelectorAll("main section");

      const observer = new IntersectionObserver(entries => {
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            entry.target.classList.add("visible");
          }
        });
      }, { threshold: 0.5 });

      sections.forEach(section => observer.observe(section));
    });

    const wordPool = [
    { scrambled: "YMNOEK", answer: "monkey", hint: "An animal" },
    { scrambled: "RLEPA", answer: "pearl", hint: "Found in oysters" },
    { scrambled: "LPEAP", answer: "apple", hint: "A fruit" },
    { scrambled: "RFUOL", answer: "flour", hint: "Used to make bread" },
    { scrambled: "AAANBN", answer: "banana", hint: "A fruit" },
    ];

    let currentWord = {};

    function getRandomWord() {
      const index = Math.floor(Math.random() * wordPool.length);
      return wordPool[index];
    }

    function openModal() {
      document.getElementById('cvModal').style.display = 'block';
      const unlocked = sessionStorage.getItem("cvUnlocked") === "true";

      if (unlocked) {
        // Show CV directly
        document.getElementById("cvLink").style.display = "block";
        document.getElementById("cvSuccess").style.display = "block";
        document.querySelector(".cv-submit").style.display = "none";
        document.getElementById("cvAnswer").style.display = "none";
        document.getElementById("cvError").style.display = "none";
      } else {
        // Pick a new random word
        currentWord = getRandomWord();
        document.querySelector(".prompt").textContent = `Unscramble the word: ${currentWord.scrambled}`;
        document.getElementById("cvAnswer").value = '';
        document.getElementById("cvError").style.display = "none";
        document.getElementById("cvSuccess").style.display = "none";
        document.getElementById("cvLink").style.display = "none";
        document.querySelector(".cv-submit").style.display = "inline-block";
        document.getElementById("cvAnswer").style.display = "inline-block";
      }
    }

    function checkCVAnswer() {
      const answer = document.getElementById("cvAnswer").value.trim().toLowerCase();

      if (answer === currentWord.answer.toLowerCase()) {
        sessionStorage.setItem("cvUnlocked", "true");

        document.getElementById("cvLink").style.display = "block";
        document.getElementById("cvError").style.display = "none";
        document.getElementById("cvSuccess").style.display = "block";
        document.querySelector(".cv-submit").style.display = "none";
        document.getElementById("cvAnswer").style.display = "none";
      } else {
        document.getElementById("cvError").textContent = `Try again! Hint: ${currentWord.hint}`;
        document.getElementById("cvError").style.display = "block";
        document.getElementById("cvLink").style.display = "none";
        document.getElementById("cvSuccess").style.display = "none";
      }
    }
    
    function closeModal() {
    document.getElementById('cvModal').style.display = 'none';
    }

    // Close modal via x button
    document.getElementById('cvClose').addEventListener('click', closeModal);
    
    // Close modal on outside click
    window.onclick = function(event) {
      const modal = document.getElementById('cvModal');
      if (event.target == modal) {
        modal.style.display = "none";
      }
    }

    // Back to Top Button
    const backToTop = document.getElementById("backToTop");
    window.onscroll = function () {
      backToTop.style.display = window.scrollY > 300 ? "block" : "none";
    };

    backToTop.onclick = function () {
      window.scrollTo({ top: 0, behavior: "smooth" });
    };
  </script>

  <div class="badge-base LI-profile-badge" 
     data-locale="en_US" 
     data-size="medium" 
     data-theme="light" 
     data-type="VERTICAL" 
     data-vanity="chengyu-cathy-fan-38578b245" 
     data-version="v1">
  <a class="badge-base__link LI-simple-link" 
     href="https://www.linkedin.com/in/chengyu-cathy-fan-38578b245?trk=profile-badge">
     Chengyu (Cathy) Fan
  </a>
</div>

<script src="https://platform.linkedin.com/badges/js/profile.js" async defer></script>



</body>
</html>
